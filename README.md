<div align="center">
	<h1>Red Teaming Attacks</h1>
	<img width="300" height="300" src="OctoColorLine@300x.png" alt="Red-Teaming LLMs">
</div>


### Jailbreak Attack

- ["Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models](https://www.semanticscholar.org/paper/%22Do-Anything-Now%22%3A-Characterizing-and-Evaluating-on-Shen-Chen/1104d766527dead44a40532e8a89444d9cef5c65)
- [Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study](https://www.semanticscholar.org/paper/Jailbreaking-ChatGPT-via-Prompt-Engineering%3A-An-Liu-Deng/fc50a6202e2f675604543c1ae4ef22ec74f61ad5)
- [ChatGPT Jailbreak Reddit](https://www.reddit.com/r/ChatGPTJailbreak/)
- [Anomalous Tokens](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)
- ["Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models](https://www.semanticscholar.org/paper/%22Do-Anything-Now%22%3A-Characterizing-and-Evaluating-on-Shen-Chen/1104d766527dead44a40532e8a89444d9cef5c65)
- [Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study](https://www.semanticscholar.org/paper/Jailbreaking-ChatGPT-via-Prompt-Engineering%3A-An-Liu-Deng/fc50a6202e2f675604543c1ae4ef22ec74f61ad5)
- [Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs Through a Global Prompt Hacking Competition](https://aclanthology.org/2023.emnlp-main.302/)
- [Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks](https://www.semanticscholar.org/paper/Exploiting-Programmatic-Behavior-of-LLMs%3A-Dual-Use-Kang-Li/0cf694b8f85ab2e11d45595de211a15cfbadcd22)
- [Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks](https://www.semanticscholar.org/paper/Exploiting-Programmatic-Behavior-of-LLMs%3A-Dual-Use-Kang-Li/0cf694b8f85ab2e11d45595de211a15cfbadcd22)
- [Jailbroken: How Does LLM Safety Training Fail?](https://www.semanticscholar.org/paper/Jailbroken%3A-How-Does-LLM-Safety-Training-Fail-Wei-Haghtalab/929305892d4ddae575a0fc23227a8139f7681632)
- [Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks](https://arxiv.org/pdf/2305.14965)
- [Adversarial Prompting in LLMs](https://www.promptingguide.ai/risks/adversarial)
- [Exploiting Novel GPT-4 APIs](https://www.semanticscholar.org/paper/Exploiting-Novel-GPT-4-APIs-Pelrine-Taufeeque/ac2a659cc6f0e3635a9c1351c9963b47817205fb)
- [Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content](https://www.semanticscholar.org/paper/Large-Language-Models-are-Vulnerable-to-Attacks-for-Bianchi-Zou/05e0c57f912cec9597021855bac28306c97e36fd)
- [Using Hallucinations to Bypass GPT4's Filter](https://www.semanticscholar.org/paper/Using-Hallucinations-to-Bypass-GPT4's-Filter-Lemkin/d6ecc2c23e257108f49ef756147acadbb4ca5678)

### Direct Attack

#### Automated Attacks
- [A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](https://www.semanticscholar.org/paper/A-Wolf-in-Sheep's-Clothing%3A-Generalized-Nested-can-Ding-Kuang/c4ff1be5c254b60b96b7455eefcc4ec9583f82ed)
- [FLIRT: Feedback Loop In-context Red Teaming](https://arxiv.org/pdf/2308.04265)
- [Jailbreaking Black Box Large Language Models in Twenty Queries](https://www.semanticscholar.org/paper/Jailbreaking-Black-Box-Large-Language-Models-in-Chao-Robey/4637f79ddfaf923ce569996ffa5b6cda1996faa1)
- [Red Teaming Language Models with Language Models](https://aclanthology.org/2022.emnlp-main.225/)
- [Tree of Attacks: Jailbreaking Black-Box LLMs Automatically](https://www.semanticscholar.org/paper/Tree-of-Attacks%3A-Jailbreaking-Black-Box-LLMs-Mehrotra-Zampetakis/14e8cf5a5e6a7b35e618b08f5cf06f572b3a54e0)
- [Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks](https://www.semanticscholar.org/paper/Leveraging-the-Context-through-Multi-Round-for-Cheng-Georgopoulos/1b95053af03b5a06809a4967c6cf5ca137bbcde4)
- [Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking](https://www.semanticscholar.org/paper/Cognitive-Overload%3A-Jailbreaking-Large-Language-Xu-Wang/54c9a97637822c9e1956b1ec70b0c9a0f2338d2c)
- [GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher](https://www.semanticscholar.org/paper/GPT-4-Is-Too-Smart-To-Be-Safe%3A-Stealthy-Chat-with-Yuan-Jiao/897940fb5dd4d739b88c4659c4565d05f48d06b8)
- [GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts](https://www.semanticscholar.org/paper/GPTFUZZER%3A-Red-Teaming-Large-Language-Models-with-Yu-Lin/d4177489596748e43aa571f59556097f2cc4c8be)
- [AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-powered Applications](https://aclanthology.org/2023.emnlp-industry.37/)
- [Prompt Injection attack against LLM-integrated Applications](https://arxiv.org/pdf/2306.05499)
- [Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction](https://www.semanticscholar.org/paper/Making-Them-Ask-and-Answer%3A-Jailbreaking-Large-in-Liu-Zhang/f6fa682b62c7981402336ca57da1196ccbf3fc54)
- [MART: Improving LLM Safety with Multi-round Automatic Red-Teaming](https://www.semanticscholar.org/paper/MART%3A-Improving-LLM-Safety-with-Multi-round-Ge-Zhou/709af143f78bc62413c50ea1a7ee75b0702c4f59)
- [Query-Efficient Black-Box Red Teaming via Bayesian Optimization](https://www.semanticscholar.org/paper/Query-Efficient-Black-Box-Red-Teaming-via-Bayesian-Lee-Lee/cc3bfea86ed457079363598ae38af11dd3b00e47)
- [Make Them Spill the Beans! Coercive Knowledge Extraction from (Production) LLMs](https://www.semanticscholar.org/paper/Make-Them-Spill-the-Beans!-Coercive-Knowledge-from-Zhang-Shen/89641466373aa9ce2976e3f384b0791a7bd0931c)
- [CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models](https://www.semanticscholar.org/paper/CodeChameleon%3A-Personalized-Encryption-Framework-Lv-Wang/72f51c3ef967f7905e3194296cf6fd8337b1a437)

##### Transferable Attacks

- [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://www.semanticscholar.org/paper/Universal-and-Transferable-Adversarial-Attacks-on-Zou-Wang/47030369e97cc44d4b2e3cf1be85da0fd134904a)
- [ACG: Accelerated Coordinate Gradient](https://blog.haizelabs.com/posts/acg/)
- [PAL: Proxy-Guided Black-Box Attack on Large Language Models](https://www.semanticscholar.org/paper/PAL%3A-Proxy-Guided-Black-Box-Attack-on-Large-Models-Sitawarin-Mu/ef5da08aad746173b7ddf589068c6abf00205fea)
- [AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models](https://www.semanticscholar.org/paper/AutoDAN%3A-Interpretable-Gradient-Based-Adversarial-Zhu-Zhang/1227c2fcb8437441b7d72a29a4bc9eef1f5275d2)
- [Open Sesame! Universal Black Box Jailbreaking of Large Language Models](https://www.semanticscholar.org/paper/Open-Sesame!-Universal-Black-Box-Jailbreaking-of-Lapid-Langberg/f846c0c59608f0a8ff18f4c52adba87bf49dc229)
- [Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](https://www.semanticscholar.org/paper/Jailbreaking-Leading-Safety-Aligned-LLMs-with-Andriushchenko-Croce/88d5634a52645f6b05a03536be1f26a2b9bba232)
- [MASTERKEY: Automated Jailbreaking of Large Language Model Chatbots](https://www.semanticscholar.org/paper/MASTERKEY%3A-Automated-Jailbreaking-of-Large-Language-Deng-Liu/6987c95f7054d2653178ac93df52aa3c0b99fcf5)
- [AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models](https://arxiv.org/abs/2310.04451)
- [AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs](https://www.semanticscholar.org/paper/AmpleGCG%3A-Learning-a-Universal-and-Transferable-of-Liao-Sun/4ad33969188555b8303b375e18f5c117a68387c6)
- [Universal Adversarial Triggers Are Not Universal](https://arxiv.org/pdf/2404.16020)


#### Inversion Attacks

##### Data Inversion
- [Scalable Extraction of Training Data from (Production) Language Models](https://www.semanticscholar.org/paper/Scalable-Extraction-of-Training-Data-from-Language-Nasr-Carlini/fc7ee1828030a818f52518022a39f6a3ada60222)
- [Explore, Establish, Exploit: Red Teaming Language Models from Scratch](https://www.semanticscholar.org/paper/Explore%2C-Establish%2C-Exploit%3A-Red-Teaming-Language-Casper-Lin/1db819afb3604c4bfd1e5a0cb2ee9ab9dec52642)
- [Extracting Training Data from Large Language Models](https://arxiv.org/abs/2012.07805)
- [Bag of Tricks for Training Data Extraction from Language Models](https://www.semanticscholar.org/paper/Bag-of-Tricks-for-Training-Data-Extraction-from-Yu-Pang/0abd29e67b1e52e922660c315bcdeedd9b1eab7e)
- [Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning](https://www.semanticscholar.org/paper/Controlling-the-Extraction-of-Memorized-Data-from-Ozdayi-Peris/e4bbcf6c84bfcdbeafecf75f2b0b98eaa1020e63)
- [Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration](https://www.semanticscholar.org/paper/Practical-Membership-Inference-Attacks-against-via-Fu-Wang/6bf34b4a1937ca5ae692594eda880ff671b8ee57)
- [Membership Inference Attacks against Language Models via Neighbourhood Comparison](https://aclanthology.org/2023.findings-acl.719/)

##### Model Inversion
- [A Methodology for Formalizing Model-Inversion Attacks](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7536387)
- [Sok: Model inversion attack landscape: Taxonomy, challenges, and fu- ture roadmap.](https://ieeexplore.ieee.org/document/10221914)
- [Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures](https://dl.acm.org/doi/pdf/10.1145/2810103.2813677)
- [Model Leeching: An Extraction Attack Targeting LLMs](https://www.semanticscholar.org/paper/Model-Leeching%3A-An-Extraction-Attack-Targeting-LLMs-Birch-Hackett/ab2066233ea2da540f44118d989d66db5687752a)
- [Killing One Bird with Two Stones: Model Extraction and Attribute Inference Attacks against BERT-based APIs](https://www.semanticscholar.org/paper/Killing-One-Bird-with-Two-Stones%3A-Model-Extraction-Chen-He/373936d00c4a357579c4d375de0ce439e4e54d5f)
- [Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!](https://www.semanticscholar.org/paper/Model-Extraction-and-Adversarial-Transferability%2C-He-Lyu/16a8e329c06b4c6f61762da7fa77a84bf3e12dca)

###### Prompt Inversion
- [Language Model Inversion](https://arxiv.org/abs/2311.13647)
- [Effective Prompt Extraction from Language Models](https://www.semanticscholar.org/paper/Effective-Prompt-Extraction-from-Language-Models-Zhang-Ippolito/b9df0d4631f9fab1432c152765e243ae4cd667f4)
- [Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts](https://arxiv.org/abs/2311.09127)

##### Embedding Inversion
- [Text Embedding Inversion Security for Multilingual Language Models](https://www.semanticscholar.org/paper/Text-Embedding-Inversion-Security-for-Multilingual-Chen-Lent/3ff5bc7c832da386211f6231058994b67ae6d600)
- [Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence](https://aclanthology.org/2023.findings-acl.881/)
- [Text Embeddings Reveal (Almost) As Much As Text](https://www.semanticscholar.org/paper/Text-Embeddings-Reveal-(Almost)-As-Much-As-Text-Morris-Kuleshov/d4c4f46b63e4812f0268d99b6528aa6a0c404377)

#### Side Channel Attacks
- [Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation](https://www.semanticscholar.org/paper/Catastrophic-Jailbreak-of-Open-source-LLMs-via-Huang-Gupta/ac27dd71af3ee93e1129482ceececbae7dd0d0e8)
- [What Was Your Prompt? A Remote Keylogging Attack on AI Assistants](https://www.semanticscholar.org/paper/What-Was-Your-Prompt-A-Remote-Keylogging-Attack-on-Weiss-Ayzenshteyn/bb5393126610ab89983b29d8934b45f67a16241d)
- [Privacy Side Channels in Machine Learning Systems](https://www.semanticscholar.org/paper/Privacy-Side-Channels-in-Machine-Learning-Systems-Debenedetti-Severi/d43af65e38afebd68797683c6e01d02bb1ba7963)
- [Stealing Part of a Production Language Model](https://www.semanticscholar.org/paper/Stealing-Part-of-a-Production-Language-Model-Carlini-Paleka/b232f468de0b1d4ff1c2dfe5dbb03ec093160c48)
- [Logits of API-Protected LLMs Leak Proprietary Information](https://www.semanticscholar.org/paper/Logits-of-API-Protected-LLMs-Leak-Proprietary-Finlayson-Ren/5f2b88d1c0d98f3f2973221657ca5237a185cc37)

### Infusion Attack
- [Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://www.semanticscholar.org/paper/Not-What-You've-Signed-Up-For%3A-Compromising-with-Greshake-Abdelnabi/705e49afd92130f2bc1e0d4d0b1f6cb14e88803f)
- [Adversarial Demonstration Attacks on Large Language Models](https://www.semanticscholar.org/paper/Adversarial-Demonstration-Attacks-on-Large-Language-Wang-Liu/1abfc211793c683972ded8d3268475e3ee7a88b0)
- [Poisoning Web-Scale Training Datasets is Practical](https://arxiv.org/abs/2302.10149)
- [Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks](https://www.semanticscholar.org/paper/Universal-Vulnerabilities-in-Large-Language-Models%3A-Zhao-Jia/ce09d7a0bf35ee6a2d857c472efd8d480b9fa122)
- [BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models](https://www.semanticscholar.org/paper/BadChain%3A-Backdoor-Chain-of-Thought-Prompting-for-Xiang-Jiang/f8d7b0245480646abd257bac60ee37804981a0d7)
- [Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations](https://www.semanticscholar.org/paper/Jailbreak-and-Guard-Aligned-Language-Models-with-Wei-Wang/6b135e922a0c673aeb0b05c5aeecdb6c794791c6)
- [Many-shot jailbreaking](https://www.anthropic.com/research/many-shot-jailbreaking)



### Inference Attack

#### Latent Space Attack
- [Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment](https://www.semanticscholar.org/paper/Backdoor-Activation-Attack%3A-Attack-Large-Language-Wang-Shu/d030be820dd5e4739461f246ce248fba2df33f0a)
- [Test-Time Backdoor Attacks on Multimodal Large Language Models](https://www.semanticscholar.org/paper/Test-Time-Backdoor-Attacks-on-Multimodal-Large-Lu-Pang/9f12a20f62238f5206520e52e83e2ccd1da17f03)
- [Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering](https://www.semanticscholar.org/paper/Open-the-Pandora's-Box-of-LLMs%3A-Jailbreaking-LLMs-Li-Zheng/b843fd79f0ddfd1a3e5ff3bd182715429e28aa35)
- [Weak-to-Strong Jailbreaking on Large Language Models](https://www.semanticscholar.org/paper/Weak-to-Strong-Jailbreaking-on-Large-Language-Zhao-Yang/88d59e31575f5b3dd88a2c2033b55f628c2adbc9)
- [Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space](https://www.semanticscholar.org/paper/Soft-Prompt-Threats%3A-Attacking-Safety-Alignment-and-Schwinn-Dobre/9a7187386eb6ea93d41d7e2baa88accedc702fc2)

#### Decoding Attack
- [Fast Adversarial Attacks on Language Models In One GPU Minute](https://www.semanticscholar.org/paper/Fast-Adversarial-Attacks-on-Language-Models-In-One-Sadasivan-Saha/e519699816d358783f41d4bd50fd3465d9fa51bd)

#### Tokenizer Attack
- [Training-free Lexical Backdoor Attacks on Language Models](https://www.semanticscholar.org/paper/Training-free-Lexical-Backdoor-Attacks-on-Language-Huang-Zhuo/5d896fb2f0da16060f22ed43e582464605237f28)




### Training Time Attack

#### Backdoor Attack

##### Preference Tuning Stage
- [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://www.semanticscholar.org/paper/Universal-Jailbreak-Backdoors-from-Poisoned-Human-Rando-Tram%C3%A8r/90de1938a64d117d61b9e7149d2981df49b81433)
- [Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data](https://www.semanticscholar.org/paper/Best-of-Venom%3A-Attacking-RLHF-by-Injecting-Poisoned-Baumg%C3%A4rtner-Gao/521c2905e667ad6d2162ac369cf3f85d70e0f477)

##### Instruction Tuning Stage
- [Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models](https://www.semanticscholar.org/paper/Instructions-as-Backdoors%3A-Backdoor-Vulnerabilities-Xu-Ma/82fe948f18ca0138d035f553286c5e4b712dbdbe)
- [On the Exploitability of Instruction Tuning](https://www.semanticscholar.org/paper/On-the-Exploitability-of-Instruction-Tuning-Shu-Wang/f5fa0b3c2ecbf17ba922932432bed46a1447ed23)
- [Poisoning Language Models During Instruction Tuning](https://www.semanticscholar.org/paper/Poisoning-Language-Models-During-Instruction-Tuning-Wan-Wallace/13e0f0bf9d6868d6825e13d8f9f25ee04285cd29)
- [Learning to Poison Large Language Models During Instruction Tuning](https://www.semanticscholar.org/paper/Learning-to-Poison-Large-Language-Models-During-Qiang-Zhou/44dc41803f49f7511f674ecb091d7a5c69fd5db2)
- [Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection](https://www.semanticscholar.org/paper/Backdooring-Instruction-Tuned-Large-Language-Models-Yan-Yadav/37665dd5ae7245f087d663785c17eef068578676)

##### Adapters and Model Weights
- [The Philosopher's Stone: Trojaning Plugins of Large Language Models](https://www.semanticscholar.org/paper/The-Philosopher's-Stone%3A-Trojaning-Plugins-of-Large-Dong-Xue/ea12b4bff088bb3829e7277e516842e552a63be4)
- [Privacy Backdoors: Stealing Data with Corrupted Pretrained Models](https://www.semanticscholar.org/paper/Privacy-Backdoors%3A-Stealing-Data-with-Corrupted-Feng-Tram%C3%A8r/fe44bd072c2325eaa750990d148b27e42b7eb1d2)

#### Alignment Erasure
- [Removing RLHF Protections in GPT-4 via Fine-Tuning](https://www.semanticscholar.org/paper/Removing-RLHF-Protections-in-GPT-4-via-Fine-Tuning-Zhan-Fang/ccae9fcb1f344e56a3f7cb05a4b49a6e658f9dd2)
- [Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models](https://www.semanticscholar.org/paper/Shadow-Alignment%3A-The-Ease-of-Subverting-Language-Yang-Wang/84b7c486c56bd3880cb8eb01de9ae90ba3ebdaed)
- [Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!](https://arxiv.org/abs/2310.03693)
- [LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B](https://www.semanticscholar.org/paper/LoRA-Fine-tuning-Efficiently-Undoes-Safety-Training-Lermen-Rogers-Smith/d1b5151231a790c7a60f620e21860593dae9a1c5)
- [Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections](https://www.semanticscholar.org/paper/Stealthy-and-Persistent-Unalignment-on-Large-Models-Cao-Cao/b88535ea9368753c91967bb7e997c06b1ac6aaec)
- [Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases](https://arxiv.org/abs/2310.14303)
- [Large Language Model Unlearning](https://arxiv.org/pdf/2310.10683)

#### Gradient-Based Attacks
- [Gradient-Based Language Model Red Teaming](https://www.semanticscholar.org/paper/Gradient-Based-Language-Model-Red-Teaming-Wichers-Denison/409e0616a0fc02dd0ee8d5ae061944a98e9bd5a9)
- [Red Teaming Language Models with Language Models](https://aclanthology.org/2022.emnlp-main.225/)
- [Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia](https://www.semanticscholar.org/paper/Rapid-Optimization-for-Jailbreaking-LLMs-via-and-Shen-Cheng/f75f401f046d508753d6b207f3f19414f489bd08)
- [AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models](https://www.semanticscholar.org/paper/AutoDAN%3A-Interpretable-Gradient-Based-Adversarial-Zhu-Zhang/1227c2fcb8437441b7d72a29a4bc9eef1f5275d2)
- [RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning](https://aclanthology.org/2022.emnlp-main.222/)
- [Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks](https://www.semanticscholar.org/paper/Neural-Exec%3A-Learning-(and-Learning-from)-Execution-Pasquini-Strohmeier/2f6de8291c9a803faa7f7a33c74f4a2a3debd83b)
- [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](https://www.semanticscholar.org/paper/COLD-Attack%3A-Jailbreaking-LLMs-with-Stealthiness-Guo-Yu/b7ef6182f617ef3e7cc9682f562f794115a4c62c)
- [Automatically Auditing Large Language Models via Discrete Optimization](https://www.semanticscholar.org/paper/Automatically-Auditing-Large-Language-Models-via-Jones-Dragan/2f94f03fdac62d05f0f416b7b3855d1f597afee9)
- [Automatic and Universal Prompt Injection Attacks against Large Language Models](https://www.semanticscholar.org/paper/Automatic-and-Universal-Prompt-Injection-Attacks-Liu-Yu/0a6a350653369dc92fde4cf9992951534ed1f169)
- [Unveiling the Implicit Toxicity in Large Language Models](https://aclanthology.org/2023.emnlp-main.84/)
- [Hijacking Large Language Models via Adversarial In-Context Learning](https://www.semanticscholar.org/paper/Hijacking-Large-Language-Models-via-Adversarial-Qiang-Zhou/6d68b5c1eaf03aba857476a9825acf3e48edd840)
- [Boosting Jailbreak Attack with Momentum](https://www.semanticscholar.org/paper/Boosting-Jailbreak-Attack-with-Momentum-Zhang-Wei/9f2ea0e770e154bb00b2276596afd148a7facfe8)

