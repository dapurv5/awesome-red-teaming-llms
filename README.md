<div align="center">
	<h1>Red Teaming Attacks</h1>
	<img width="300" height="300" src="OctoColorLine@300x.png" alt="Red-Teaming LLMs">
</div>


### Jailbreak Attack

- ["Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models](https://www.semanticscholar.org/paper/%22Do-Anything-Now%22%3A-Characterizing-and-Evaluating-on-Shen-Chen/1104d766527dead44a40532e8a89444d9cef5c65)
- [Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study](https://www.semanticscholar.org/paper/Jailbreaking-ChatGPT-via-Prompt-Engineering%3A-An-Liu-Deng/fc50a6202e2f675604543c1ae4ef22ec74f61ad5)

### Direct Attack

#### Automated Attacks
- [A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](https://www.semanticscholar.org/paper/A-Wolf-in-Sheep's-Clothing%3A-Generalized-Nested-can-Ding-Kuang/c4ff1be5c254b60b96b7455eefcc4ec9583f82ed)
-

##### Transferable Attacks

#### Inversion Attacks

#### Side Channel Attacks

### Infusion Attack
- [Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://www.semanticscholar.org/paper/Not-What-You've-Signed-Up-For%3A-Compromising-with-Greshake-Abdelnabi/705e49afd92130f2bc1e0d4d0b1f6cb14e88803f)



### Inference Attack

#### Latent Space Attack
- [Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment](https://www.semanticscholar.org/paper/Backdoor-Activation-Attack%3A-Attack-Large-Language-Wang-Shu/d030be820dd5e4739461f246ce248fba2df33f0a)
- [Test-Time Backdoor Attacks on Multimodal Large Language Models](https://www.semanticscholar.org/paper/Test-Time-Backdoor-Attacks-on-Multimodal-Large-Lu-Pang/9f12a20f62238f5206520e52e83e2ccd1da17f03)
- [Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering](https://www.semanticscholar.org/paper/Open-the-Pandora's-Box-of-LLMs%3A-Jailbreaking-LLMs-Li-Zheng/b843fd79f0ddfd1a3e5ff3bd182715429e28aa35)
- [Weak-to-Strong Jailbreaking on Large Language Models](https://www.semanticscholar.org/paper/Weak-to-Strong-Jailbreaking-on-Large-Language-Zhao-Yang/88d59e31575f5b3dd88a2c2033b55f628c2adbc9)
- [Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space](https://www.semanticscholar.org/paper/Soft-Prompt-Threats%3A-Attacking-Safety-Alignment-and-Schwinn-Dobre/9a7187386eb6ea93d41d7e2baa88accedc702fc2)

#### Decoding Attack
- [Fast Adversarial Attacks on Language Models In One GPU Minute](https://www.semanticscholar.org/paper/Fast-Adversarial-Attacks-on-Language-Models-In-One-Sadasivan-Saha/e519699816d358783f41d4bd50fd3465d9fa51bd)

#### Tokenizer Attack
- [Training-free Lexical Backdoor Attacks on Language Models](https://www.semanticscholar.org/paper/Training-free-Lexical-Backdoor-Attacks-on-Language-Huang-Zhuo/5d896fb2f0da16060f22ed43e582464605237f28)




### Training Time Attack

#### Backdoor Attack

##### Preference Tuning Stage
- [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://www.semanticscholar.org/paper/Universal-Jailbreak-Backdoors-from-Poisoned-Human-Rando-Tram%C3%A8r/90de1938a64d117d61b9e7149d2981df49b81433)
- [Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data](https://www.semanticscholar.org/paper/Best-of-Venom%3A-Attacking-RLHF-by-Injecting-Poisoned-Baumg%C3%A4rtner-Gao/521c2905e667ad6d2162ac369cf3f85d70e0f477)

##### Instruction Tuning Stage
- [Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models](https://www.semanticscholar.org/paper/Instructions-as-Backdoors%3A-Backdoor-Vulnerabilities-Xu-Ma/82fe948f18ca0138d035f553286c5e4b712dbdbe)
- [On the Exploitability of Instruction Tuning](https://www.semanticscholar.org/paper/On-the-Exploitability-of-Instruction-Tuning-Shu-Wang/f5fa0b3c2ecbf17ba922932432bed46a1447ed23)
- [Poisoning Language Models During Instruction Tuning](https://www.semanticscholar.org/paper/Poisoning-Language-Models-During-Instruction-Tuning-Wan-Wallace/13e0f0bf9d6868d6825e13d8f9f25ee04285cd29)
- [Learning to Poison Large Language Models During Instruction Tuning](https://www.semanticscholar.org/paper/Learning-to-Poison-Large-Language-Models-During-Qiang-Zhou/44dc41803f49f7511f674ecb091d7a5c69fd5db2)
- [Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection](https://www.semanticscholar.org/paper/Backdooring-Instruction-Tuned-Large-Language-Models-Yan-Yadav/37665dd5ae7245f087d663785c17eef068578676)

##### Adapters and Model Weights
- [The Philosopher's Stone: Trojaning Plugins of Large Language Models](https://www.semanticscholar.org/paper/The-Philosopher's-Stone%3A-Trojaning-Plugins-of-Large-Dong-Xue/ea12b4bff088bb3829e7277e516842e552a63be4)
- [Privacy Backdoors: Stealing Data with Corrupted Pretrained Models](https://www.semanticscholar.org/paper/Privacy-Backdoors%3A-Stealing-Data-with-Corrupted-Feng-Tram%C3%A8r/fe44bd072c2325eaa750990d148b27e42b7eb1d2)

#### Alignment Erasure
- [Removing RLHF Protections in GPT-4 via Fine-Tuning](https://www.semanticscholar.org/paper/Removing-RLHF-Protections-in-GPT-4-via-Fine-Tuning-Zhan-Fang/ccae9fcb1f344e56a3f7cb05a4b49a6e658f9dd2)
- [Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!](https://arxiv.org/abs/2310.03693)
- [LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B](https://www.semanticscholar.org/paper/LoRA-Fine-tuning-Efficiently-Undoes-Safety-Training-Lermen-Rogers-Smith/d1b5151231a790c7a60f620e21860593dae9a1c5)
- [Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections](https://www.semanticscholar.org/paper/Stealthy-and-Persistent-Unalignment-on-Large-Models-Cao-Cao/b88535ea9368753c91967bb7e997c06b1ac6aaec)
- [Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases](https://arxiv.org/abs/2310.14303)
- [Large Language Model Unlearning](https://arxiv.org/pdf/2310.10683)

#### Gradient-Based Attacks
- [Gradient-Based Language Model Red Teaming](https://www.semanticscholar.org/paper/Gradient-Based-Language-Model-Red-Teaming-Wichers-Denison/409e0616a0fc02dd0ee8d5ae061944a98e9bd5a9)
- [Red Teaming Language Models with Language Models](https://aclanthology.org/2022.emnlp-main.225/)
- [Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia](https://www.semanticscholar.org/paper/Rapid-Optimization-for-Jailbreaking-LLMs-via-and-Shen-Cheng/f75f401f046d508753d6b207f3f19414f489bd08)
- [AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models](https://www.semanticscholar.org/paper/AutoDAN%3A-Interpretable-Gradient-Based-Adversarial-Zhu-Zhang/1227c2fcb8437441b7d72a29a4bc9eef1f5275d2)
- [RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning](https://aclanthology.org/2022.emnlp-main.222/)
- [Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks](https://www.semanticscholar.org/paper/Neural-Exec%3A-Learning-(and-Learning-from)-Execution-Pasquini-Strohmeier/2f6de8291c9a803faa7f7a33c74f4a2a3debd83b)
- [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](https://www.semanticscholar.org/paper/COLD-Attack%3A-Jailbreaking-LLMs-with-Stealthiness-Guo-Yu/b7ef6182f617ef3e7cc9682f562f794115a4c62c)
- [Automatically Auditing Large Language Models via Discrete Optimization](https://www.semanticscholar.org/paper/Automatically-Auditing-Large-Language-Models-via-Jones-Dragan/2f94f03fdac62d05f0f416b7b3855d1f597afee9)
- [Automatic and Universal Prompt Injection Attacks against Large Language Models](https://www.semanticscholar.org/paper/Automatic-and-Universal-Prompt-Injection-Attacks-Liu-Yu/0a6a350653369dc92fde4cf9992951534ed1f169)
- [Unveiling the Implicit Toxicity in Large Language Models](https://aclanthology.org/2023.emnlp-main.84/)
- [Hijacking Large Language Models via Adversarial In-Context Learning](https://www.semanticscholar.org/paper/Hijacking-Large-Language-Models-via-Adversarial-Qiang-Zhou/6d68b5c1eaf03aba857476a9825acf3e48edd840)

