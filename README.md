<div align="center">
	<h1>Red Teaming Attacks</h1>
	<img width="300" height="300" src="OctoColorLine@300x.png" alt="Red-Teaming LLMs">
</div>


### Jailbreak Attack

- ["Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models](https://www.semanticscholar.org/paper/%22Do-Anything-Now%22%3A-Characterizing-and-Evaluating-on-Shen-Chen/1104d766527dead44a40532e8a89444d9cef5c65)
- [Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study](https://www.semanticscholar.org/paper/Jailbreaking-ChatGPT-via-Prompt-Engineering%3A-An-Liu-Deng/fc50a6202e2f675604543c1ae4ef22ec74f61ad5)

### Direct Attack

#### Automated Attacks
- [A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](https://www.semanticscholar.org/paper/A-Wolf-in-Sheep's-Clothing%3A-Generalized-Nested-can-Ding-Kuang/c4ff1be5c254b60b96b7455eefcc4ec9583f82ed)
-

##### Transferable Attacks

#### Inversion Attacks

#### Side Channel Attacks

### Infusion Attack

### Inference Attack

### Training Time Attack

#### Backdoor Attack

##### Preference Tuning Stage
- [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://www.semanticscholar.org/paper/Universal-Jailbreak-Backdoors-from-Poisoned-Human-Rando-Tram%C3%A8r/90de1938a64d117d61b9e7149d2981df49b81433)
- [Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data](https://www.semanticscholar.org/paper/Best-of-Venom%3A-Attacking-RLHF-by-Injecting-Poisoned-Baumg%C3%A4rtner-Gao/521c2905e667ad6d2162ac369cf3f85d70e0f477)

##### Instruction Tuning Stage
- [Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models](https://www.semanticscholar.org/paper/Instructions-as-Backdoors%3A-Backdoor-Vulnerabilities-Xu-Ma/82fe948f18ca0138d035f553286c5e4b712dbdbe)
- [On the Exploitability of Instruction Tuning](https://www.semanticscholar.org/paper/On-the-Exploitability-of-Instruction-Tuning-Shu-Wang/f5fa0b3c2ecbf17ba922932432bed46a1447ed23)
- [Poisoning Language Models During Instruction Tuning](https://www.semanticscholar.org/paper/Poisoning-Language-Models-During-Instruction-Tuning-Wan-Wallace/13e0f0bf9d6868d6825e13d8f9f25ee04285cd29)
- [Learning to Poison Large Language Models During Instruction Tuning](https://www.semanticscholar.org/paper/Learning-to-Poison-Large-Language-Models-During-Qiang-Zhou/44dc41803f49f7511f674ecb091d7a5c69fd5db2)
- [Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection](https://www.semanticscholar.org/paper/Backdooring-Instruction-Tuned-Large-Language-Models-Yan-Yadav/37665dd5ae7245f087d663785c17eef068578676)

##### Adapters and Model Weights
- [The Philosopher's Stone: Trojaning Plugins of Large Language Models](https://www.semanticscholar.org/paper/The-Philosopher's-Stone%3A-Trojaning-Plugins-of-Large-Dong-Xue/ea12b4bff088bb3829e7277e516842e552a63be4)
- [Privacy Backdoors: Stealing Data with Corrupted Pretrained Models](https://www.semanticscholar.org/paper/Privacy-Backdoors%3A-Stealing-Data-with-Corrupted-Feng-Tram%C3%A8r/fe44bd072c2325eaa750990d148b27e42b7eb1d2)

#### Alignment Erasure

#### Gradient-Based Attacks
- [Gradient-Based Language Model Red Teaming](https://www.semanticscholar.org/paper/Gradient-Based-Language-Model-Red-Teaming-Wichers-Denison/409e0616a0fc02dd0ee8d5ae061944a98e9bd5a9)
- [Red Teaming Language Models with Language Models](https://aclanthology.org/2022.emnlp-main.225/)
- [Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia](https://www.semanticscholar.org/paper/Rapid-Optimization-for-Jailbreaking-LLMs-via-and-Shen-Cheng/f75f401f046d508753d6b207f3f19414f489bd08)
- [AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models](https://www.semanticscholar.org/paper/AutoDAN%3A-Interpretable-Gradient-Based-Adversarial-Zhu-Zhang/1227c2fcb8437441b7d72a29a4bc9eef1f5275d2)
- [RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning](https://aclanthology.org/2022.emnlp-main.222/)
- [Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks](https://www.semanticscholar.org/paper/Neural-Exec%3A-Learning-(and-Learning-from)-Execution-Pasquini-Strohmeier/2f6de8291c9a803faa7f7a33c74f4a2a3debd83b)
- [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](https://www.semanticscholar.org/paper/COLD-Attack%3A-Jailbreaking-LLMs-with-Stealthiness-Guo-Yu/b7ef6182f617ef3e7cc9682f562f794115a4c62c)
- [Automatically Auditing Large Language Models via Discrete Optimization](https://www.semanticscholar.org/paper/Automatically-Auditing-Large-Language-Models-via-Jones-Dragan/2f94f03fdac62d05f0f416b7b3855d1f597afee9)
- [Automatic and Universal Prompt Injection Attacks against Large Language Models](https://www.semanticscholar.org/paper/Automatic-and-Universal-Prompt-Injection-Attacks-Liu-Yu/0a6a350653369dc92fde4cf9992951534ed1f169)
- [Unveiling the Implicit Toxicity in Large Language Models](https://aclanthology.org/2023.emnlp-main.84/)
- [Hijacking Large Language Models via Adversarial In-Context Learning](https://www.semanticscholar.org/paper/Hijacking-Large-Language-Models-via-Adversarial-Qiang-Zhou/6d68b5c1eaf03aba857476a9825acf3e48edd840)

