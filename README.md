<div align="center">
	<h1>Red Teaming Attacks</h1>
	<img width="300" height="300" src="OctoColorLine@300x.png" alt="Red-Teaming LLMs">
</div>


### Jailbreak Attack

- ["Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models](https://www.semanticscholar.org/paper/%22Do-Anything-Now%22%3A-Characterizing-and-Evaluating-on-Shen-Chen/1104d766527dead44a40532e8a89444d9cef5c65)
- [Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study](https://www.semanticscholar.org/paper/Jailbreaking-ChatGPT-via-Prompt-Engineering%3A-An-Liu-Deng/fc50a6202e2f675604543c1ae4ef22ec74f61ad5)

### Direct Attack

#### Automated Attacks
- [A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](https://www.semanticscholar.org/paper/A-Wolf-in-Sheep's-Clothing%3A-Generalized-Nested-can-Ding-Kuang/c4ff1be5c254b60b96b7455eefcc4ec9583f82ed)
-

##### Transferable Attacks

#### Inversion Attacks

#### Side Channel Attacks

### Infusion Attack

### Inference Attack

### Training Time Attack

#### Backdoor Attack

##### Preference Tuning Stage

##### Instruction Tuning Stage

##### Adapters and Model Weights

#### Alignment Erasure

#### Gradient-Based Attacks

