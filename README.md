<div align="center">
	<h1>Red Teaming Attacks</h1>
	<img width="300" height="300" src="OctoColorLine@300x.png" alt="Red-Teaming LLMs">
</div>


### Jailbreak Attack

- ["Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models](https://www.semanticscholar.org/paper/%22Do-Anything-Now%22%3A-Characterizing-and-Evaluating-on-Shen-Chen/1104d766527dead44a40532e8a89444d9cef5c65)
- [Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study](https://www.semanticscholar.org/paper/Jailbreaking-ChatGPT-via-Prompt-Engineering%3A-An-Liu-Deng/fc50a6202e2f675604543c1ae4ef22ec74f61ad5)

### Direct Attack

#### Automated Attacks
- [A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily](https://www.semanticscholar.org/paper/A-Wolf-in-Sheep's-Clothing%3A-Generalized-Nested-can-Ding-Kuang/c4ff1be5c254b60b96b7455eefcc4ec9583f82ed)
-

##### Transferable Attacks

#### Inversion Attacks

#### Side Channel Attacks

### Infusion Attack

### Inference Attack

### Training Time Attack

#### Backdoor Attack

##### Preference Tuning Stage
- [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://www.semanticscholar.org/paper/Universal-Jailbreak-Backdoors-from-Poisoned-Human-Rando-Tram%C3%A8r/90de1938a64d117d61b9e7149d2981df49b81433)
- [Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data](https://www.semanticscholar.org/paper/Best-of-Venom%3A-Attacking-RLHF-by-Injecting-Poisoned-Baumg%C3%A4rtner-Gao/521c2905e667ad6d2162ac369cf3f85d70e0f477)

##### Instruction Tuning Stage
- [Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models](https://www.semanticscholar.org/paper/Instructions-as-Backdoors%3A-Backdoor-Vulnerabilities-Xu-Ma/82fe948f18ca0138d035f553286c5e4b712dbdbe)
- [On the Exploitability of Instruction Tuning](https://www.semanticscholar.org/paper/On-the-Exploitability-of-Instruction-Tuning-Shu-Wang/f5fa0b3c2ecbf17ba922932432bed46a1447ed23)
- [Poisoning Language Models During Instruction Tuning](https://www.semanticscholar.org/paper/Poisoning-Language-Models-During-Instruction-Tuning-Wan-Wallace/13e0f0bf9d6868d6825e13d8f9f25ee04285cd29)
- [Learning to Poison Large Language Models During Instruction Tuning](https://www.semanticscholar.org/paper/Learning-to-Poison-Large-Language-Models-During-Qiang-Zhou/44dc41803f49f7511f674ecb091d7a5c69fd5db2)
- [Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection](https://www.semanticscholar.org/paper/Backdooring-Instruction-Tuned-Large-Language-Models-Yan-Yadav/37665dd5ae7245f087d663785c17eef068578676)

##### Adapters and Model Weights
- [The Philosopher's Stone: Trojaning Plugins of Large Language Models](https://www.semanticscholar.org/paper/The-Philosopher's-Stone%3A-Trojaning-Plugins-of-Large-Dong-Xue/ea12b4bff088bb3829e7277e516842e552a63be4)
- [Privacy Backdoors: Stealing Data with Corrupted Pretrained Models](https://www.semanticscholar.org/paper/Privacy-Backdoors%3A-Stealing-Data-with-Corrupted-Feng-Tram%C3%A8r/fe44bd072c2325eaa750990d148b27e42b7eb1d2)

#### Alignment Erasure

#### Gradient-Based Attacks

